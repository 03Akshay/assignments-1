{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789569ff-b28f-4a10-9263-8045831b3a7f",
   "metadata": {},
   "source": [
    "# #Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5949f3b-80eb-403d-b456-5f4f3458c42a",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It is an extension of the Gradient Boosting Machine (GBM) algorithm, which is a powerful ensemble learning method. Gradient Boosting Regression builds multiple weak learners (usually decision trees) sequentially, and each new learner is trained to correct the errors of the previous ones.\n",
    "\n",
    "Here's a high-level overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialization**: It starts with an initial prediction, which is often the average of the target variable for the entire dataset.\n",
    "\n",
    "2. **Residual Calculation**: The algorithm calculates the residuals (the differences between the actual target values and the predictions made so far).\n",
    "\n",
    "3. **Building Weak Learners**: A weak learner, typically a decision tree, is trained to predict the residuals. The decision tree is often shallow, with limited depth to avoid overfitting.\n",
    "\n",
    "4. **Weighted Summation**: The predictions from the weak learner are multiplied by a learning rate (also known as the shrinkage parameter) and added to the current prediction. The learning rate controls the contribution of each weak learner to the final prediction, and it helps in preventing overfitting.\n",
    "\n",
    "5. **Update Target**: The target for the next iteration is updated by subtracting the previous predictions and adding the newly calculated residuals.\n",
    "\n",
    "6. **Iteration**: Steps 2 to 5 are repeated iteratively, with each new weak learner trying to correct the errors made by the ensemble of previous learners.\n",
    "\n",
    "7. **Final Prediction**: The final prediction is obtained by summing up the predictions from all the weak learners.\n",
    "\n",
    "Gradient Boosting Regression is a powerful algorithm that can capture complex relationships in the data and is less prone to overfitting compared to individual decision trees. However, it requires tuning of hyperparameters, such as the number of weak learners, learning rate, and the depth of the individual trees, to achieve optimal performance on a specific dataset. The most popular implementation of Gradient Boosting Regression is the XGBoost library, although there are other implementations like LightGBM and CatBoost, each with its strengths and optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d90679-534d-41ae-87d5-7bb504cc2ce4",
   "metadata": {},
   "source": [
    "# #Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7645713-0d57-47c9-8cf1-652e1b647aa9",
   "metadata": {},
   "source": [
    "Sure! Let's implement a simple Gradient Boosting Regression algorithm from scratch using Python and NumPy. We'll create a basic decision tree as the weak learner and use mean squared error (MSE) and R-squared as evaluation metrics. For demonstration purposes, we'll use a small dataset.\n",
    "\n",
    "Let's start by importing the necessary libraries and defining the helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10e0f55-c73d-4ce7-b828-46cfb5f3c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the decision tree class (weak learner)\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def mse(self, y):\n",
    "        return np.mean((y - np.mean(y))**2)\n",
    "\n",
    "    def split(self, X, y, feature_idx, threshold):\n",
    "        left_mask = X[:, feature_idx] < threshold\n",
    "        X_left, y_left = X[left_mask], y[left_mask]\n",
    "        X_right, y_right = X[~left_mask], y[~left_mask]\n",
    "        return X_left, X_right, y_left, y_right\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        best_mse = float('inf')\n",
    "        best_feature_idx, best_threshold = None, None\n",
    "\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            unique_values = np.unique(X[:, feature_idx])\n",
    "            for threshold in unique_values:\n",
    "                X_left, X_right, y_left, y_right = self.split(X, y, feature_idx, threshold)\n",
    "                mse_left = self.mse(y_left)\n",
    "                mse_right = self.mse(y_right)\n",
    "                total_mse = mse_left + mse_right\n",
    "                if total_mse < best_mse:\n",
    "                    best_mse = total_mse\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_idx, best_threshold\n",
    "\n",
    "    def build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or self.mse(y) == 0:\n",
    "            return np.mean(y)\n",
    "\n",
    "        feature_idx, threshold = self.find_best_split(X, y)\n",
    "        X_left, X_right, y_left, y_right = self.split(X, y, feature_idx, threshold)\n",
    "\n",
    "        tree = {\n",
    "            'feature_idx': feature_idx,\n",
    "            'threshold': threshold,\n",
    "            'left': self.build_tree(X_left, y_left, depth + 1),\n",
    "            'right': self.build_tree(X_right, y_right, depth + 1)\n",
    "        }\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(x, self.tree) for x in X])\n",
    "\n",
    "    def _predict(self, x, tree):\n",
    "        if isinstance(tree, float):\n",
    "            return tree\n",
    "        feature_val = x[tree['feature_idx']]\n",
    "        if feature_val < tree['threshold']:\n",
    "            return self._predict(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict(x, tree['right'])\n",
    "\n",
    "\n",
    "# Define the Gradient Boosting Regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.zeros(len(y))\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            residuals = y - y_pred\n",
    "            tree.fit(X, residuals)\n",
    "            update = tree.predict(X)\n",
    "            y_pred += self.learning_rate * update\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd30cd-fb25-4920-96c2-697d3ab320de",
   "metadata": {},
   "source": [
    "Now, let's create a small dataset, train the Gradient Boosting Regression model on it, and evaluate its performance using mean squared error (MSE) and R-squared:\n",
    "\n",
    "python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7efb3f-5606-4a41-b29e-8eebe357e480",
   "metadata": {},
   "source": [
    "# Create a small dataset\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Initialize and train the Gradient Boosting Regression model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance on the training data\n",
    "y_pred_train = gb_model.predict(X_train)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = np.mean((y_train - y_pred_train) ** 2)\n",
    "\n",
    "# Calculate R-squared\n",
    "ss_total = np.sum((y_train - np.mean(y_train)) ** 2)\n",
    "ss_residual = np.sum((y_train - y_pred_train) ** 2)\n",
    "r_squared = 1 - (ss_residual / ss_total)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61654470-d164-4753-a0a9-4086616241dd",
   "metadata": {},
   "source": [
    "# #Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb694000-f3ec-4c29-80eb-b3cf78dcc8c0",
   "metadata": {},
   "source": [
    "Sure! We can use grid search to experiment with different hyperparameters such as learning rate, the number of trees (n_estimators), and tree depth (max_depth) to optimize the performance of the Gradient Boosting Regression model. Grid search will exhaustively try all possible combinations of hyperparameters within specified ranges and help us find the best combination.\n",
    "\n",
    "Here's how you can perform grid search using scikit-learn library in Python:\n",
    "\n",
    "python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbae75f-c16d-430d-9e30-e444e2b42127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "Number of Trees (n_estimators): 50\n",
      "Learning Rate: 0.1\n",
      "Max Depth of Trees (max_depth): 2\n",
      "\n",
      "Test Set Performance:\n",
      "Mean Squared Error (MSE): 37.75259869003385\n",
      "R-squared: 0.9749625749709511\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create a small dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Gradient Boosting Regression model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameters grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "best_learning_rate = grid_search.best_params_['learning_rate']\n",
    "best_max_depth = grid_search.best_params_['max_depth']\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Number of Trees (n_estimators):\", best_n_estimators)\n",
    "print(\"Learning Rate:\", best_learning_rate)\n",
    "print(\"Max Depth of Trees (max_depth):\", best_max_depth)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_gb_model = GradientBoostingRegressor(n_estimators=best_n_estimators, learning_rate=best_learning_rate, max_depth=best_max_depth)\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808981d-fa54-4d68-8c11-0ebe1e27ba76",
   "metadata": {},
   "source": [
    "In this example, we use scikit-learn's GridSearchCV to perform grid search over the specified hyperparameter ranges. The cv=5 parameter in GridSearchCV specifies a 5-fold cross-validation, and we use negative mean squared error as the scoring metric since GridSearchCV tries to maximize the scoring metric.\n",
    "\n",
    "After grid search, we obtain the best hyperparameters and create a new Gradient Boosting Regression model with those hyperparameters. Finally, we evaluate the performance of the model on the test set using mean squared error (MSE) and R-squared. The best hyperparameters found through grid search should result in a more optimized model for the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac3cd9-cad4-4be7-afa4-2966186b02f4",
   "metadata": {},
   "source": [
    "# #Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ae86e-f775-4c10-9566-dad49665b0e2",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a simple and relatively simple model that performs only slightly better than random guessing on a given learning task. In most cases, weak learners are decision trees with limited depth, also known as \"stumps\" or \"shallow trees.\"\n",
    "\n",
    "The concept of using weak learners in the Gradient Boosting algorithm is essential for its effectiveness. Gradient Boosting works by combining multiple weak learners to create a strong ensemble model that can make accurate predictions.\n",
    "\n",
    "The idea is that although an individual weak learner might not be very accurate on its own, it can still capture some patterns or relationships in the data. When these weak learners are combined, the ensemble model can collectively learn and represent more complex patterns and relationships, leading to improved predictive performance.\n",
    "\n",
    "During each iteration of the Gradient Boosting algorithm, a new weak learner is trained to correct the errors of the previous ensemble of weak learners. The new weak learner is fitted to the residuals (the differences between the actual target values and the predictions made by the current ensemble model).\n",
    "\n",
    "The process of sequentially adding weak learners and adjusting their weights is where the \"boosting\" in Gradient Boosting comes into play. Each new learner focuses on the mistakes made by the previous learners, gradually refining the model's predictions and reducing the overall error.\n",
    "\n",
    "The use of weak learners, along with the boosting process, makes Gradient Boosting powerful and capable of handling complex and high-dimensional datasets while mitigating the risk of overfitting. It is a widely used and successful algorithm for various machine learning tasks, including regression, classification, and ranking problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f018d0-a12d-41a6-b4fd-69e3dd00bc14",
   "metadata": {},
   "source": [
    "# #Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ec903-1731-4eb7-b1dc-b153d2bd4d62",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood through the following key concepts:\n",
    "\n",
    "1. **Ensemble Learning**: Gradient Boosting is an ensemble learning technique, which means it combines the predictions of multiple weak learners (typically decision trees) to create a more accurate and robust model. The basic idea is that by combining the predictions of several models, the ensemble can learn more complex patterns and reduce the individual models' errors.\n",
    "\n",
    "2. **Boosting**: The term \"boosting\" refers to the iterative process of sequentially adding weak learners to the ensemble, where each new learner is trained to correct the errors made by the previous ensemble. In other words, it focuses on the data points that were misclassified or poorly predicted by the previous models.\n",
    "\n",
    "3. **Residual Learning**: The core idea of Gradient Boosting is residual learning. During each iteration, the new weak learner is trained to predict the residuals (the differences between the actual target values and the predictions made by the current ensemble). By fitting the weak learner to the residuals, the algorithm aims to reduce the errors of the previous ensemble.\n",
    "\n",
    "4. **Gradient Descent**: The term \"Gradient\" in Gradient Boosting refers to the use of gradient descent optimization to minimize the loss function. Instead of directly updating the model's parameters, Gradient Boosting adjusts the parameters in the direction that minimizes the loss function (e.g., mean squared error) for the current iteration.\n",
    "\n",
    "5. **Learning Rate**: Gradient Boosting introduces a learning rate (also called the shrinkage parameter) to control the contribution of each weak learner to the ensemble's final prediction. A smaller learning rate prevents overfitting by reducing the impact of each learner, while a larger learning rate allows the model to learn faster but may lead to overfitting.\n",
    "\n",
    "The intuition can be summarized as follows: In each iteration, Gradient Boosting fits a weak learner to the negative gradient of the loss function (residuals) to update the model's predictions. By gradually improving the predictions with each new learner, the ensemble gets closer to the optimal prediction function, reducing the overall prediction error.\n",
    "\n",
    "The iterative nature of the algorithm allows it to learn complex relationships in the data, making it highly effective for various machine learning tasks, including regression, classification, and ranking. Gradient Boosting has been widely used in practice due to its high predictive performance and ability to handle large and high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbd1ec-8c2a-48ba-b7de-c3d85890aa9f",
   "metadata": {},
   "source": [
    "# #Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e380484-395e-43d1-8e42-193fdd15a16f",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood through the following key concepts:\n",
    "\n",
    "1. **Ensemble Learning**: Gradient Boosting is an ensemble learning technique, which means it combines the predictions of multiple weak learners (typically decision trees) to create a more accurate and robust model. The basic idea is that by combining the predictions of several models, the ensemble can learn more complex patterns and reduce the individual models' errors.\n",
    "\n",
    "2. **Boosting**: The term \"boosting\" refers to the iterative process of sequentially adding weak learners to the ensemble, where each new learner is trained to correct the errors made by the previous ensemble. In other words, it focuses on the data points that were misclassified or poorly predicted by the previous models.\n",
    "\n",
    "3. **Residual Learning**: The core idea of Gradient Boosting is residual learning. During each iteration, the new weak learner is trained to predict the residuals (the differences between the actual target values and the predictions made by the current ensemble). By fitting the weak learner to the residuals, the algorithm aims to reduce the errors of the previous ensemble.\n",
    "\n",
    "4. **Gradient Descent**: The term \"Gradient\" in Gradient Boosting refers to the use of gradient descent optimization to minimize the loss function. Instead of directly updating the model's parameters, Gradient Boosting adjusts the parameters in the direction that minimizes the loss function (e.g., mean squared error) for the current iteration.\n",
    "\n",
    "5. **Learning Rate**: Gradient Boosting introduces a learning rate (also called the shrinkage parameter) to control the contribution of each weak learner to the ensemble's final prediction. A smaller learning rate prevents overfitting by reducing the impact of each learner, while a larger learning rate allows the model to learn faster but may lead to overfitting.\n",
    "\n",
    "The intuition can be summarized as follows: In each iteration, Gradient Boosting fits a weak learner to the negative gradient of the loss function (residuals) to update the model's predictions. By gradually improving the predictions with each new learner, the ensemble gets closer to the optimal prediction function, reducing the overall prediction error.\n",
    "\n",
    "The iterative nature of the algorithm allows it to learn complex relationships in the data, making it highly effective for various machine learning tasks, including regression, classification, and ranking. Gradient Boosting has been widely used in practice due to its high predictive performance and ability to handle large and high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9d021-0585-4b4c-9812-71b43eca8f76",
   "metadata": {},
   "source": [
    "# #Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0ad75-c109-47bb-bab4-1aa0c910cf5d",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the key mathematical concepts and optimization techniques used during its training process. Here are the main steps involved in developing the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. **Loss Function**: The first step is to define a suitable loss function, which quantifies the difference between the predicted values and the actual target values. For regression tasks, the mean squared error (MSE) is commonly used as the loss function. For classification tasks, the cross-entropy loss or log loss is often used.\n",
    "\n",
    "2. **Initialize Ensemble**: The algorithm starts with an initial prediction, usually set to the average (or any other appropriate value) of the target variable for the entire training dataset.\n",
    "\n",
    "3. **Gradient Descent Optimization**: Gradient Boosting optimizes the loss function using gradient descent. The idea is to update the model's parameters in the direction that minimizes the loss function. In each iteration, the negative gradient of the loss function (with respect to the predicted values) is computed for the current ensemble's predictions.\n",
    "\n",
    "4. **Residual Calculation**: The negative gradient, also known as the \"residuals\" or \"pseudo-residuals,\" represents the errors made by the current ensemble on the training data. These residuals indicate how far off the current predictions are from the true target values.\n",
    "\n",
    "5. **Training Weak Learners**: A weak learner, often a decision tree with limited depth, is trained to predict the residuals. The tree is typically shallow to prevent overfitting and to ensure that it focuses on capturing the patterns in the residuals.\n",
    "\n",
    "6. **Learning Rate and Weighted Summation**: The predictions from the new weak learner are multiplied by a learning rate (shrinkage parameter) and added to the current prediction made by the ensemble. The learning rate controls the contribution of each weak learner to the final prediction. Smaller learning rates make the algorithm more conservative, preventing overfitting.\n",
    "\n",
    "7. **Update Target**: The target for the next iteration is updated by subtracting the previous predictions and adding the newly calculated residuals. This new target is used to train the next weak learner, which aims to correct the errors made by the current ensemble.\n",
    "\n",
    "8. **Iteration**: Steps 3 to 7 are repeated iteratively for a predefined number of iterations (n_estimators). In each iteration, a new weak learner is trained to predict the updated residuals and added to the ensemble. The process gradually improves the ensemble's performance, reducing the overall prediction error.\n",
    "\n",
    "9. **Final Prediction**: The final prediction is obtained by summing up the predictions from all the weak learners in the ensemble.\n",
    "\n",
    "By following these steps and optimizing the loss function using gradient descent, the Gradient Boosting algorithm effectively constructs an ensemble of weak learners that work together to make accurate predictions on the given learning task. The mathematical intuition helps us understand how the algorithm learns from the data and how each component contributes to the overall model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c15dce-e5a3-419b-bcb4-7292bf37939c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
