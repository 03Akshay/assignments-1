{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334e2740-fef2-439b-8250-f52c123a9d6a",
   "metadata": {},
   "source": [
    "Q1. You are work#ng on a mach#ne learn#ng project where you have a dataset conta#n#ng numer#cal and\n",
    "categor#cal features. You have #dent#f#ed that some of the features are h#ghly correlated and there are\n",
    "m#ss#ng values #n some of the columns. You want to bu#ld a p#pel#ne that automates the feature\n",
    "eng#neer#ng process and handles the m#ss#ng valuesD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6accbf-426f-498c-b830-a4e9c83932ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assume you have loaded your dataset into a pandas DataFrame called \"data\"\n",
    "# Separate the target variable and features\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Step 1: Automated Feature Selection\n",
    "# Use Random Forest to select important features\n",
    "rf_feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "X_selected = rf_feature_selector.fit_transform(X, y)\n",
    "\n",
    "# Step 2: Numerical Pipeline\n",
    "# Create numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 3: Categorical Pipeline\n",
    "# Create categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Step 4: Combine Numerical and Categorical Pipelines\n",
    "# Use ColumnTransformer to combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', num_pipeline, X.columns[~X.columns.isin(X.select_dtypes(include='object').columns)]),\n",
    "    ('categorical', cat_pipeline, X.select_dtypes(include='object').columns)\n",
    "])\n",
    "\n",
    "# Step 5: Final Model Building\n",
    "# Create the final pipeline with the preprocessor and Random Forest Classifier\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Step 6: Train-Test Split\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Fit and Evaluate the Model\n",
    "# Fit the model on the training data\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240fac71-7d45-46bf-8623-33482116867a",
   "metadata": {},
   "source": [
    "Explanation of each step:\n",
    "\n",
    "Automated Feature Selection: We use a Random Forest Classifier as a feature selector to identify important features from the dataset. This helps us reduce the dimensionality and focus on the most relevant features.\n",
    "\n",
    "Numerical Pipeline: The numerical pipeline consists of an imputer that fills missing values in numerical columns with the mean of the column values, and then scales the numerical columns using standardization to bring them to a common scale.\n",
    "\n",
    "Categorical Pipeline: The categorical pipeline includes an imputer that fills missing values in categorical columns with the most frequent value and then one-hot encodes the categorical columns to convert them into numerical representation.\n",
    "\n",
    "Combine Numerical and Categorical Pipelines: We use ColumnTransformer to combine the numerical and categorical pipelines, handling both types of features simultaneously.\n",
    "\n",
    "Final Model Building: We create the final pipeline by combining the preprocessor (numerical and categorical pipelines) with the Random Forest Classifier as the final model.\n",
    "\n",
    "Train-Test Split: We split the data into training and test sets to evaluate the model's performance.\n",
    "\n",
    "Fit and Evaluate the Model: We fit the final pipeline on the training data and evaluate the accuracy of the model on the test data using the accuracy_score metric.\n",
    "\n",
    "Interpretation of Results:\n",
    "The accuracy obtained on the test dataset will give us an estimate of the model's performance. If the accuracy is high, it indicates that the pipeline is performing well on unseen data. If the accuracy is low, we may need to investigate further, tune hyperparameters, or explore other models.\n",
    "\n",
    "Possible Improvements:\n",
    "\n",
    "Hyperparameter Tuning: We can perform hyperparameter tuning on the Random Forest Classifier and other model-related hyperparameters to further improve the model's performance.\n",
    "\n",
    "Cross-Validation: Instead of a simple train-test split, we can use cross-validation for more reliable performance evaluation and parameter tuning.\n",
    "\n",
    "Feature Engineering: We can explore additional feature engineering techniques to create new meaningful features that might enhance the model's performance.\n",
    "\n",
    "Ensemble Methods: We can experiment with other ensemble methods like Gradient Boosting, AdaBoost, or XGBoost, and compare their performances with the Random Forest.\n",
    "\n",
    "Handling Imbalanced Data: If the dataset is imbalanced, we may need to apply techniques to handle class imbalance, such as oversampling, undersampling, or using different class weights.\n",
    "\n",
    "It's important to note that the success of the pipeline depends on the characteristics of the data and the problem at hand. Continuous refinement and experimentation are essential for building robust and high-performing machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b51409-eba7-49c6-b9d4-4a3b37e9d120",
   "metadata": {},
   "source": [
    "# #Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
    "use a vot#ng class#f#er to comb#ne the#r pred#ct#ons. Tra#n the p#pel#ne on the #r#s dataset and evaluate #ts\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018189f-8d21-4a0e-a853-1d0286995d9c",
   "metadata": {},
   "source": [
    "Sure! We can build a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions. Let's assume we have a binary classification problem. We'll use scikit-learn for the implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7057597-473a-40f6-98da-61fe00075388",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assume you have loaded your dataset into a pandas DataFrame called \"data\"\n",
    "# Separate the target variable and features\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Step 1: Create the individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "logreg_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Step 2: Create the Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_classifier),\n",
    "    ('logreg', logreg_classifier)\n",
    "], voting='hard')\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Fit and Evaluate the Voting Classifier\n",
    "# Fit the voting classifier on the training data\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6197d-852c-458b-87c9-eacef637d367",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "1. We create the individual classifiers: Random Forest Classifier (`rf_classifier`) and Logistic Regression Classifier (`logreg_classifier`).\n",
    "\n",
    "2. We then create the Voting Classifier (`voting_classifier`) and pass the individual classifiers as a list of tuples, where the first element of each tuple is a string identifier for the classifier, and the second element is the classifier object itself. The `voting` parameter is set to `'hard'`, which means the final prediction will be the majority vote among the individual classifiers.\n",
    "\n",
    "3. We split the data into training and test sets.\n",
    "\n",
    "4. We fit the voting classifier on the training data and predict on the test data to evaluate its accuracy.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The accuracy obtained on the test dataset will give us an estimate of the voting classifier's performance, which combines the predictions of the Random Forest Classifier and Logistic Regression Classifier. The voting classifier can potentially improve the overall accuracy by leveraging the strengths of both individual classifiers. If the accuracy is high, it indicates that the ensemble of classifiers is performing well on unseen data. If the accuracy is low, we may need to investigate further, tune hyperparameters, or try different combinations of classifiers.\n",
    "\n",
    "Keep in mind that the success of the voting classifier depends on the individual classifiers' performances and their diversity. If the individual classifiers are complementary and make different types of errors, the voting classifier is more likely to perform well.\n",
    "\n",
    "Also, note that the choice of classifiers and the number of classifiers in the ensemble can be adjusted based on the specific dataset and problem at hand. Further hyperparameter tuning and feature engineering can also be explored to optimize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5659c6-8488-4088-b971-27d48f2a2652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
