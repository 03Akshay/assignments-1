{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d499bf-d8d7-4b38-9da3-5b665d4df341",
   "metadata": {},
   "source": [
    "# #Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add0010-bbb2-4752-b6df-e1cc7bc31b1b",
   "metadata": {},
   "source": [
    "In the context of artificial neural networks, an activation function is a non-linear mathematical function that determines the output of a neuron (or node) given its weighted sum of inputs. Activation functions introduce non-linearity to the network, allowing it to learn and approximate complex relationships in data. Without activation functions, the neural network would behave like a linear model, regardless of its depth and complexity.\n",
    "\n",
    "The activation function takes the weighted sum of inputs, also known as the \"activation\" or \"logit,\" and transforms it into the output of the neuron. This output is then used as input for subsequent layers or as the final prediction of the network.\n",
    "\n",
    "Activation functions are applied element-wise to the weighted sum, meaning they are applied individually to each neuron's input. The choice of activation function can have a significant impact on the network's learning capabilities, convergence speed, and overall performance.\n",
    "\n",
    "Key roles and characteristics of activation functions include:\n",
    "\n",
    "1. **Introducing Non-Linearity:** Activation functions introduce non-linearity to the network, enabling it to capture complex patterns in data that linear functions cannot.\n",
    "\n",
    "2. **Learning Complex Relationships:** Non-linear activation functions allow the network to learn intricate and higher-level features present in the data.\n",
    "\n",
    "3. **Gradient Flow:** Activation functions impact the flow of gradients during backpropagation, which is crucial for training neural networks using gradient descent optimization algorithms.\n",
    "\n",
    "4. **Handling Negative Values:** Some activation functions, like Rectified Linear Unit (ReLU) and its variants, handle negative values in ways that allow gradients to flow more effectively.\n",
    "\n",
    "5. **Output Interpretability:** The type of activation function used in the output layer of the network depends on the task. For instance, sigmoid and softmax functions are often used for classification tasks where output represents probabilities.\n",
    "\n",
    "6. **Avoiding Saturation:** Saturation refers to the situation where the function approaches its upper or lower bounds, causing gradients to become very small. Some activation functions, like sigmoid and tanh, can suffer from saturation in certain input ranges.\n",
    "\n",
    "Commonly used activation functions include sigmoid, tanh, ReLU, Leaky ReLU, Parametric ReLU (PReLU), Exponential Linear Unit (ELU), and more. The choice of activation function depends on the specific task, network architecture, and considerations such as avoiding vanishing gradients and achieving faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d54bb0-1831-4cd0-ab9a-76e773d82cba",
   "metadata": {},
   "source": [
    "# #Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860d451-a87d-4b99-b221-d276aeebea0f",
   "metadata": {},
   "source": [
    "There are several types of activation functions commonly used in neural networks. Each activation function introduces non-linearity into the network, allowing it to learn complex relationships in data. Here are some of the most common types of activation functions:\n",
    "\n",
    "1. **Sigmoid:**\n",
    "   - The sigmoid activation function maps input values to the range [0, 1].\n",
    "   - It's often used in the output layer for binary classification tasks where the output represents probabilities.\n",
    "   - \\( f(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "\n",
    "2. **Hyperbolic Tangent (tanh):**\n",
    "   - The tanh activation function maps input values to the range [-1, 1].\n",
    "   - Like the sigmoid, it saturates for large positive or negative inputs, but its output is zero-centered.\n",
    "   - \\( f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
    "\n",
    "3. **Rectified Linear Unit (ReLU):**\n",
    "   - The ReLU activation function outputs the input for positive values and zero for negative values.\n",
    "   - It helps mitigate the vanishing gradient problem and accelerates training in deep networks.\n",
    "   - \\( f(x) = \\max(0, x) \\)\n",
    "\n",
    "4. **Leaky ReLU:**\n",
    "   - Leaky ReLU is a variant of ReLU that allows a small gradient for negative inputs.\n",
    "   - This addresses the \"dying ReLU\" problem where ReLU neurons become inactive during training.\n",
    "   - \\( f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha x, & \\text{if } x \\leq 0 \\end{cases} \\), where \\( \\alpha \\) is a small positive constant.\n",
    "\n",
    "5. **Parametric ReLU (PReLU):**\n",
    "   - PReLU is an extension of Leaky ReLU where the slope for negative inputs is learned during training.\n",
    "   - This allows the network to adaptively adjust the slope of the activation function.\n",
    "   - \\( f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha x, & \\text{if } x \\leq 0 \\end{cases} \\), where \\( \\alpha \\) is a learnable parameter.\n",
    "\n",
    "6. **Exponential Linear Unit (ELU):**\n",
    "   - ELU is similar to ReLU for positive inputs but has a non-zero slope for negative inputs.\n",
    "   - It helps mitigate the vanishing gradient problem and allows negative values without outputting zero.\n",
    "   - \\( f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha (e^x - 1), & \\text{if } x \\leq 0 \\end{cases} \\), where \\( \\alpha \\) is a small positive constant.\n",
    "\n",
    "7. **Scaled Exponential Linear Unit (SELU):**\n",
    "   - SELU is a variant of ELU that has a special property that can lead to self-normalization in deep networks.\n",
    "   - It's designed to ensure that the activations and gradients remain close to a mean of zero and a standard deviation of one.\n",
    "   - It requires specific weight initialization and is recommended for specific architectures.\n",
    "\n",
    "These activation functions serve various purposes and offer different benefits. The choice of activation function depends on the problem you're trying to solve, the architecture of your neural network, and considerations such as vanishing gradients, output range, and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c1a99-7644-4ddb-856b-06748d61ad7e",
   "metadata": {},
   "source": [
    "# #Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9710d1a-2052-477c-a800-6c86ca4bfb68",
   "metadata": {},
   "source": [
    "Activation functions play a crucial role in the training process and overall performance of a neural network. They introduce non-linearity to the network, allowing it to learn complex relationships in data. Different activation functions have distinct effects on training convergence, gradient flow, and the network's ability to capture and represent patterns in data. Here's how activation functions impact neural network training and performance:\n",
    "\n",
    "**1. Introducing Non-Linearity:**\n",
    "   - Activation functions add non-linearity to the network, enabling it to learn and approximate complex functions. Without non-linear activation functions, a multi-layer neural network would be equivalent to a single-layer linear model.\n",
    "\n",
    "**2. Affect on Gradient Flow:**\n",
    "   - Activation functions impact the gradients that flow backward through the network during training (backpropagation). Gradients are used to update the weights in the network.\n",
    "   - Activation functions with gradients that do not vanish or explode (remain reasonable in magnitude) help address vanishing and exploding gradient problems, ensuring stable and effective training.\n",
    "\n",
    "**3. Training Speed and Convergence:**\n",
    "   - Activation functions influence the speed of training and the convergence of optimization algorithms.\n",
    "   - Activation functions that don't saturate (flatten out) for large or small inputs, like ReLU and its variants, allow for faster convergence as gradients remain informative.\n",
    "\n",
    "**4. Learning Representations:**\n",
    "   - Activation functions determine the kind of features and representations a neural network can learn from the data.\n",
    "   - Non-linear activation functions enable networks to capture intricate patterns and high-level features in the data.\n",
    "\n",
    "**5. Performance on Different Tasks:**\n",
    "   - Different activation functions are suited for different tasks. For instance, sigmoid and tanh functions can be used in the output layer for binary classification where probabilities are desired.\n",
    "   - ReLU and its variants are often used in hidden layers to enable efficient learning of complex features.\n",
    "\n",
    "**6. Avoiding Saturation:**\n",
    "   - Activation functions that saturate (flatten) for certain input ranges, like sigmoid and tanh, can suffer from slow learning in those regions. This is due to small gradients that hinder weight updates.\n",
    "\n",
    "**7. Avoiding Dead Neurons:**\n",
    "   - Activation functions that output zero for large portions of their input range (e.g., ReLU for negative inputs) can lead to \"dead neurons.\" These neurons cease to update because they consistently output zero. Techniques like Leaky ReLU help mitigate this issue.\n",
    "\n",
    "**8. Computational Efficiency:**\n",
    "   - The computational complexity of different activation functions can impact training speed. Simple functions like ReLU and its variants (e.g., Leaky ReLU) are computationally efficient compared to sigmoid and tanh.\n",
    "\n",
    "In summary, the choice of activation function has a substantial impact on the training process, convergence speed, and overall performance of a neural network. Modern architectures often use ReLU and its variants due to their favorable properties in addressing vanishing gradients, enabling faster training, and promoting the efficient learning of complex patterns. However, the selection of an appropriate activation function depends on the specific task and network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9e7b0-c5e9-4b26-9428-478863a09481",
   "metadata": {},
   "source": [
    "# #Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33886a64-d8cf-4a17-92c6-e66ce7ff9035",
   "metadata": {},
   "source": [
    "The sigmoid activation function is a commonly used non-linear function in artificial neural networks. It takes an input value and maps it to a range between 0 and 1, making it suitable for tasks that involve binary classification or probabilistic outputs. The sigmoid function's mathematical expression is:\n",
    "\n",
    "\\[ f(x) = \\frac{1}{1 + e^{-x}} \\]\n",
    "\n",
    "Where \\( x \\) is the input to the function.\n",
    "\n",
    "**How the Sigmoid Activation Function Works:**\n",
    "\n",
    "The sigmoid function takes any input value \\( x \\) and squashes it into the range [0, 1]. As \\( x \\) becomes large and positive, \\( e^{-x} \\) approaches zero, causing the denominator of the fraction to be close to 1. This results in \\( f(x) \\) being close to 1. Conversely, as \\( x \\) becomes large and negative, \\( e^{-x} \\) becomes very large, causing the denominator to approach infinity, and \\( f(x) \\) approaches 0. The sigmoid function's S-shaped curve transitions smoothly between these two extremes.\n",
    "\n",
    "**Advantages of the Sigmoid Activation Function:**\n",
    "\n",
    "1. **Bounded Output:** The output of the sigmoid function is bounded between 0 and 1, which makes it suitable for tasks where binary classification or probability estimation is required.\n",
    "\n",
    "2. **Smoothness:** The sigmoid function is smooth and continuously differentiable, which makes it compatible with gradient-based optimization algorithms used in training neural networks.\n",
    "\n",
    "3. **Interpretability:** Sigmoid outputs can be interpreted as probabilities, making it useful for tasks where probability estimates are meaningful.\n",
    "\n",
    "**Disadvantages of the Sigmoid Activation Function:**\n",
    "\n",
    "1. **Vanishing Gradient:** For very large or very small inputs, the derivative of the sigmoid function approaches zero. This leads to the vanishing gradient problem, where gradients become small during backpropagation, causing slow convergence or even stopping learning in deep networks.\n",
    "\n",
    "2. **Saturating Behavior:** The sigmoid saturates to 0 or 1 for large inputs, causing the gradients to become small. This can lead to slow learning and difficulties in optimization.\n",
    "\n",
    "3. **Output Bias:** The outputs of the sigmoid function are not zero-centered, which can lead to bias in the network's updates during training.\n",
    "\n",
    "4. **Computation Intensity:** The sigmoid function involves exponentiation and division operations, which can be computationally more intensive compared to other activation functions like ReLU.\n",
    "\n",
    "In modern neural network architectures, alternative activation functions like the Rectified Linear Unit (ReLU) and its variants are often preferred over the sigmoid function due to their ability to address issues like vanishing gradients and faster convergence. However, the sigmoid function is still used in certain contexts, such as the output layer of binary classification models and certain recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee9b37-80f7-4a6b-871b-bc84b7b167c2",
   "metadata": {},
   "source": [
    "# #Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f26ad-cfc5-494f-9dc5-b7ba9d4992f4",
   "metadata": {},
   "source": [
    "The Rectified Linear Unit (ReLU) activation function is a popular non-linear function used in neural networks and deep learning models. It's designed to introduce non-linearity into the network while addressing some of the limitations of other activation functions, such as the sigmoid function.\n",
    "\n",
    "**ReLU Activation Function:**\n",
    "The ReLU function is defined as follows:\n",
    "\\[ f(x) = \\max(0, x) \\]\n",
    "where \\(x\\) is the input to the function. In other words, if the input is positive or zero, the output is the same as the input; if the input is negative, the output is zero.\n",
    "\n",
    "**Differences between ReLU and Sigmoid:**\n",
    "\n",
    "1. **Range of Output:**\n",
    "   - Sigmoid: The sigmoid function outputs values between 0 and 1, which can represent probabilities or bounded activations.\n",
    "   - ReLU: The ReLU function outputs 0 for negative inputs and maintains the input for non-negative inputs. Its output range is from 0 to positive infinity.\n",
    "\n",
    "2. **Non-Linearity:**\n",
    "   - Sigmoid: The sigmoid function is sigmoid-shaped and introduces non-linearity in the network, which is important for capturing complex relationships in data.\n",
    "   - ReLU: ReLU is a piecewise linear function that introduces non-linearity by breaking the linearity at zero. It's computationally efficient and avoids saturation for positive inputs.\n",
    "\n",
    "3. **Vanishing Gradient:**\n",
    "   - Sigmoid: The sigmoid function saturates (approaches 0 or 1) for large positive or negative inputs, leading to small gradients and the vanishing gradient problem.\n",
    "   - ReLU: ReLU does not saturate for positive inputs, which helps mitigate the vanishing gradient problem and accelerates convergence in deep networks.\n",
    "\n",
    "4. **Computation Efficiency:**\n",
    "   - Sigmoid: Sigmoid involves expensive exponentiation and division operations, making it computationally more intensive.\n",
    "   - ReLU: ReLU involves a simple thresholding operation (max(0, x)), which is computationally efficient and faster to compute.\n",
    "\n",
    "5. **Sparsity of Activation:**\n",
    "   - Sigmoid: Sigmoid can lead to dense activations, where many neurons are active (outputting non-zero values).\n",
    "   - ReLU: ReLU can lead to sparse activations, where only a subset of neurons are active (outputting non-zero values). This can be memory-efficient.\n",
    "\n",
    "6. **Negative Inputs:**\n",
    "   - Sigmoid: Sigmoid outputs values between 0 and 1 for all inputs, including negative ones.\n",
    "   - ReLU: ReLU sets negative inputs to zero, resulting in sparsity of activations.\n",
    "\n",
    "In summary, the ReLU activation function is a simple yet effective way to introduce non-linearity in neural networks while addressing issues like vanishing gradients. It differs from the sigmoid function in terms of output range, non-linearity, computational efficiency, and handling of negative inputs. ReLU and its variants are commonly used in modern deep learning architectures due to their advantages and effectiveness in training deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bf5a6-27ab-435b-9151-5e369e713e06",
   "metadata": {},
   "source": [
    "# #Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d355d6-0855-4da2-aeaf-51763954c6d3",
   "metadata": {},
   "source": [
    "The Rectified Linear Unit (ReLU) activation function offers several benefits over the sigmoid activation function, especially in the context of training deep neural networks. Here are some key advantages of using ReLU over sigmoid:\n",
    "\n",
    "1. **Avoiding Vanishing Gradient Problem:**\n",
    "   - The sigmoid function saturates for large positive and negative inputs, causing gradients to become extremely small.\n",
    "   - This can lead to the vanishing gradient problem, where gradients approach zero, hindering learning in deep networks.\n",
    "   - ReLU addresses this issue by not saturating for positive inputs, preventing gradient vanishing.\n",
    "\n",
    "2. **Faster Convergence and Training:**\n",
    "   - ReLU's non-saturating nature accelerates the convergence of gradient-based optimization.\n",
    "   - It leads to faster training as gradients do not diminish significantly during backpropagation.\n",
    "\n",
    "3. **Sparse Activation:**\n",
    "   - Sigmoid outputs are between 0 and 1, potentially leading to dense activations where many neurons are firing.\n",
    "   - ReLU, on the other hand, outputs 0 for negative inputs, resulting in sparse activations and efficient memory usage.\n",
    "\n",
    "4. **Efficiency in Computation:**\n",
    "   - ReLU only involves a simple thresholding operation (max(0, x)) and is computationally efficient.\n",
    "   - Sigmoid involves exponentiation and division operations, which are computationally more expensive.\n",
    "\n",
    "5. **Dealing with Dead Neurons:**\n",
    "   - Sigmoid's saturating behavior can lead to \"dead\" neurons with near-zero gradients that stop updating during training.\n",
    "   - ReLU avoids this issue, as it only sets negative inputs to zero, keeping the neuron \"alive.\"\n",
    "\n",
    "6. **Natural Handling of Positive Inputs:**\n",
    "   - ReLU activation behaves similarly to how real neurons fire in response to positive stimuli.\n",
    "   - This contributes to the biological plausibility of the activation function.\n",
    "\n",
    "7. **Universal Approximation Property:**\n",
    "   - ReLU is proven to possess the universal approximation property, meaning it can approximate a wide range of functions given a sufficient number of hidden units.\n",
    "\n",
    "However, it's important to note that ReLU also has limitations:\n",
    "- The \"dying ReLU\" problem can occur if a large gradient flows through a ReLU unit during training, causing the unit to never activate again.\n",
    "- ReLU is not suitable for models that need outputs in a bounded range (e.g., when predicting probabilities).\n",
    "\n",
    "In practice, ReLU and its variants (e.g., Leaky ReLU, Parametric ReLU) are widely used in modern neural network architectures due to their effectiveness in mitigating vanishing gradients, speeding up training, and promoting better convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f481d3-f89f-44f8-846b-03a48ccdd727",
   "metadata": {},
   "source": [
    "# #Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58ac67-7b62-439e-ad63-6ea47485ec93",
   "metadata": {},
   "source": [
    "\"Leaky ReLU\" is an activation function that addresses the vanishing gradient problem associated with the standard Rectified Linear Unit (ReLU) activation function. The vanishing gradient problem occurs when the gradient of the loss function with respect to the network's weights becomes very small, leading to slow or stagnant learning during training. Leaky ReLU introduces a small slope for negative inputs, allowing a non-zero gradient to flow through the neuron even when the input is negative. This helps alleviate the vanishing gradient problem and ensures that the network can continue learning effectively.\n",
    "\n",
    "**Leaky ReLU Function:**\n",
    "The Leaky ReLU activation function is defined as follows:\n",
    "\\[ f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha x, & \\text{if } x \\leq 0 \\end{cases} \\]\n",
    "where \\( \\alpha \\) is a small positive constant, often set to a value like 0.01.\n",
    "\n",
    "**Addressing the Vanishing Gradient Problem:**\n",
    "The vanishing gradient problem occurs when gradients become very small as they are backpropagated through deep networks. This happens because the gradient of the ReLU function is zero for negative inputs, which prevents updates to the corresponding weights. As a result, the network may not learn effectively, particularly in earlier layers.\n",
    "\n",
    "Leaky ReLU addresses this problem by allowing a small gradient to flow through for negative inputs. This means that even when the input is negative, the gradient isn't entirely zero. As a result, the weights associated with those neurons can still be updated, allowing the network to learn from negative inputs as well. By introducing a small slope (\\( \\alpha \\)) for negative inputs, Leaky ReLU ensures that the network doesn't experience complete gradient vanishing.\n",
    "\n",
    "**Advantages of Leaky ReLU:**\n",
    "1. **Mitigating Dead Neurons:** Leaky ReLU helps prevent \"dead\" neurons, which are neurons that stop updating due to consistent zero outputs for negative inputs.\n",
    "2. **Addressing Gradient Vanishing:** By introducing a small non-zero gradient for negative inputs, Leaky ReLU enables better flow of gradients during backpropagation.\n",
    "3. **Effective Training:** Leaky ReLU can lead to faster and more effective training in deep networks by preventing gradient stagnation.\n",
    "\n",
    "**Considerations for Using Leaky ReLU:**\n",
    "- The value of \\( \\alpha \\) needs to be chosen carefully. A small positive value (e.g., 0.01) is commonly used, but it can be tuned based on experimentation and the specific problem.\n",
    "- Leaky ReLU may not be the best choice for every scenario. It's important to try different activation functions and architectures to determine which one works best for a given problem.\n",
    "\n",
    "In summary, Leaky ReLU is a modification of the standard ReLU activation function that introduces a small slope for negative inputs, thereby addressing the vanishing gradient problem and promoting more effective learning in deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc8a8e-5489-4a28-a0d6-69cef7d988b2",
   "metadata": {},
   "source": [
    "# #Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888585e0-15c4-4507-a2ff-ae0ac6987030",
   "metadata": {},
   "source": [
    "The softmax activation function is a widely used activation function in the context of multi-class classification tasks in neural networks. Its main purpose is to convert raw class scores, also known as logits, into a probability distribution over multiple classes. The output of the softmax function represents the estimated probability of each class, and the probabilities sum up to 1. This makes softmax suitable for tasks where the network needs to assign an input to one of several mutually exclusive classes.\n",
    "\n",
    "**Mathematical Definition of the Softmax Function:**\n",
    "Given a vector of logits \\( z = [z_1, z_2, ..., z_k] \\) for \\( k \\) classes, the softmax function calculates the probability \\( p_i \\) for class \\( i \\) as follows:\n",
    "\n",
    "\\[ p_i = \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}} \\]\n",
    "\n",
    "Where \\( e^{z_i} \\) is the exponential of the \\( i \\)-th logit, and the denominator sums up the exponentials of all logits.\n",
    "\n",
    "**Purpose and Common Use Cases:**\n",
    "\n",
    "1. **Multi-Class Classification:** The primary purpose of the softmax function is in multi-class classification tasks, where the network needs to classify inputs into one of several possible classes.\n",
    "   \n",
    "2. **Generating Class Probabilities:** The softmax function converts raw logits into class probabilities, allowing the network to provide a probability estimate for each class.\n",
    "\n",
    "3. **Decision Making:** By producing class probabilities, softmax enables decision-making processes that consider the confidence of the network's predictions.\n",
    "\n",
    "4. **Training with Cross-Entropy Loss:** The softmax function is commonly used in conjunction with the cross-entropy loss function, which measures the difference between predicted probabilities and true labels. The goal during training is to minimize this difference.\n",
    "\n",
    "5. **Ensemble Models:** In ensemble models, like softmax-based neural networks, the probabilities produced by softmax can be combined with other model outputs to make final predictions.\n",
    "\n",
    "**Common Use Cases:**\n",
    "- Image Classification: Assigning an image to one of several predefined categories.\n",
    "- Natural Language Processing: Assigning a label to a sentence or document from a set of classes.\n",
    "- Object Detection: Assigning object labels to different regions of an image.\n",
    "\n",
    "It's important to note that softmax is generally not used in isolation for binary classification tasks (two classes). In such cases, a sigmoid activation function is often used in the output layer, producing independent probabilities for each class and accommodating non-mutually exclusive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fcbc78-5d84-4340-862f-4f313a2b462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
