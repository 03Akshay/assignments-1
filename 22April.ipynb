{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1394ce-e97b-46ef-8cea-14f21e768d78",
   "metadata": {},
   "source": [
    "# #Q1. Write a Python code to implement the KNN classifier algorithm on load_iris dataset in\n",
    "sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5a36f-598e-4a0d-b89a-87e832653e89",
   "metadata": {},
   "source": [
    "Sure! The k-Nearest Neighbors (KNN) algorithm is a simple and effective classification algorithm. It's commonly used for classification tasks. Here's how you can implement the KNN classifier algorithm on the load_iris dataset from sklearn.datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705f02d-facd-4f33-a00b-4d124e4948bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dedfee-b5ab-4e79-9915-88f222ef5d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Feature scaling (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a KNN classifier object with k=3\n",
    "k = 3\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bbe972-08e5-461a-8682-126ca9a5bf6e",
   "metadata": {},
   "source": [
    "n this code, we first load the Iris dataset and split it into training and testing sets using train_test_split. Then, we create a KNN classifier with n_neighbors=3 (you can change this value to experiment with different numbers of neighbors). We train the classifier using the training data and make predictions on the test data. Finally, we calculate and print the accuracy of the KNN classifier.\n",
    "\n",
    "Please note that the Iris dataset is relatively small, and the KNN classifier is straightforward. In practice, you might want to perform additional data preprocessing, feature scaling, or use more sophisticated classification algorithms for larger and more complex datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec85cd3-25f8-4be7-b886-13d1b98fc0d1",
   "metadata": {},
   "source": [
    "# #Q2. Write a Python code to implement the KNN regressor algorithm on load_boston dataset in\n",
    "sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fceb8d-e5e8-4bb0-bd77-9a4424afdb1d",
   "metadata": {},
   "source": [
    "Sure! The k-Nearest Neighbors (KNN) regressor is a simple algorithm used for regression tasks. Let's implement the KNN regressor using the load_boston dataset from sklearn.datasets. Again, make sure you have the necessary libraries installed by running pip install scikit-learn.\n",
    "\n",
    "Here's the Python code for the KNN regressor on the Boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08358717-2529-4e83-ad4b-6fe61386910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the KNN regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)  # Set the number of neighbors (k) to 5\n",
    "\n",
    "# Train the regressor on the training data\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_regressor.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) and R-squared score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893da2a5-6d5e-4e1b-8116-19ce805d892f",
   "metadata": {},
   "source": [
    "In this code, we load the Boston dataset and split it into training and testing sets using train_test_split. Then, we create a KNN regressor with n_neighbors=5 (you can change this value to experiment with different numbers of neighbors). We train the regressor using the training data and make predictions on the test data. Finally, we calculate and print the Mean Squared Error (MSE) and R-squared score, which are commonly used evaluation metrics for regression tasks.\n",
    "\n",
    "Again, note that in practical scenarios, you may want to perform additional data preprocessing, feature scaling, or try other regression algorithms to improve performance on larger and more complex datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31306e23-a679-45ef-a1d9-9cbdd2f2a8d8",
   "metadata": {},
   "source": [
    "# #Q3. Write a Python code snippet to find the optimal value of K for the KNN classifier algorithm using\n",
    "cross-validation on load_iris dataset in sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13571b6-ee5a-4ac6-8b52-9d7a843e0090",
   "metadata": {},
   "source": [
    "To find the optimal value of K for the KNN classifier algorithm, we can use cross-validation to evaluate the model's performance for different values of K. We can use k-fold cross-validation to achieve this, where the dataset is divided into 'k' subsets (folds), and the model is trained and evaluated 'k' times, each time using a different fold as the test set and the rest as the training set.\n",
    "\n",
    "In this example, I'll use a simple loop to iterate over different values of K and perform k-fold cross-validation to find the optimal K for the KNN classifier on the Iris dataset.\n",
    "\n",
    "python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c28073-2e0c-4f7b-838f-48aee24157f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define a range of K values to try\n",
    "k_values = list(range(1, 31))\n",
    "\n",
    "# Dictionary to store cross-validation scores for each K\n",
    "k_scores = {}\n",
    "\n",
    "# Perform k-fold cross-validation for each K value\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=5)  # 5-fold cross-validation\n",
    "    k_scores[k] = np.mean(scores)\n",
    "\n",
    "# Find the optimal K with the highest mean cross-validation score\n",
    "optimal_k = max(k_scores, key=k_scores.get)\n",
    "\n",
    "print(\"Cross-validation scores for each K:\")\n",
    "for k, score in k_scores.items():\n",
    "    print(f\"K={k}, Mean Accuracy={score:.4f}\")\n",
    "\n",
    "print(\"\\nOptimal K:\", optimal_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ed7df-f4b7-46a5-8c03-ab7cc6c1aba0",
   "metadata": {},
   "source": [
    "In this code, we define a range of K values from 1 to 30. Then, we loop through each K value, create a KNN classifier with that K value, and perform 5-fold cross-validation using cross_val_score. The mean cross-validation accuracy for each K value is stored in the k_scores dictionary. Finally, we find the optimal K by selecting the K with the highest mean accuracy.\n",
    "\n",
    "Please note that the choice of the number of folds (cv=5 in this case) can vary based on the dataset size and other considerations. In practice, you can experiment with different values of cv to find the best option for your specific problem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323fecd-02dc-4bf6-9193-dfc63fbcdf41",
   "metadata": {},
   "source": [
    "# #Q4. Implement the KNN regressor algorithm with feature scaling on load_boston dataset in\n",
    "sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033dd7b-7209-4aca-bec5-175fe5621f73",
   "metadata": {},
   "source": [
    "Sure! To implement the KNN regressor algorithm with feature scaling on the `load_boston` dataset, we will use the `StandardScaler` from `sklearn.preprocessing` to scale the features before training the KNN regressor. Feature scaling is essential for distance-based algorithms like KNN to ensure that all features contribute equally to the distance calculation.\n",
    "\n",
    "Here's the Python code for the KNN regressor with feature scaling on the Boston dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features in the training and testing sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the KNN regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)  # Set the number of neighbors (k) to 5\n",
    "\n",
    "# Train the regressor on the scaled training data\n",
    "knn_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the scaled test data\n",
    "y_pred = knn_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) and R-squared score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared score:\", r2)\n",
    "```\n",
    "\n",
    "In this code, we first load the Boston dataset and split it into training and testing sets using `train_test_split`. Then, we initialize the `StandardScaler` to scale the features in the training and testing sets. We fit the scaler on the training data using `fit_transform` and transform the test data using `transform`.\n",
    "\n",
    "After scaling the features, we create a KNN regressor with `n_neighbors=5` (you can change this value to experiment with different numbers of neighbors). We train the regressor using the scaled training data and make predictions on the scaled test data. Finally, we calculate and print the Mean Squared Error (MSE) and R-squared score for the KNN regressor with feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cfcd0c-b269-449d-a125-6a920895d8fc",
   "metadata": {},
   "source": [
    "# #Q5. Write a Python code snippet to implement the KNN classifier algorithm with weighted voting on\n",
    "load_iris dataset in sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c983d7-7297-4559-8398-223396f578ba",
   "metadata": {},
   "source": [
    "To implement the KNN classifier algorithm with weighted voting, we can modify the standard KNN classifier by assigning weights to each neighbor's vote based on their distance from the query point. Closer neighbors will have a higher weight, indicating their higher importance in the voting process. We can achieve this by setting the `weights` parameter of the `KNeighborsClassifier` to `'distance'`.\n",
    "\n",
    "Here's the Python code for the KNN classifier with weighted voting on the Iris dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the KNN classifier with weighted voting\n",
    "knn = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "In this code, we load the Iris dataset and split it into training and testing sets using `train_test_split`. We create a KNN classifier with `n_neighbors=3` (you can change this value to experiment with different numbers of neighbors) and set `weights='distance'` to enable weighted voting.\n",
    "\n",
    "We then train the classifier on the training data and make predictions on the test data. Finally, we calculate and print the accuracy of the KNN classifier with weighted voting.\n",
    "\n",
    "Weighted voting allows the KNN classifier to give more importance to the closer neighbors, making it potentially more robust in certain scenarios where nearby neighbors might provide more relevant information for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90429a6-01e8-4990-ac94-56c37d857445",
   "metadata": {},
   "source": [
    "# #Q6. Implement a function to standardise the features before applying KNN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c8fc7-32f6-4fa0-be09-772b28131ed3",
   "metadata": {},
   "source": [
    "To implement a function to standardize the features before applying the KNN classifier, you can use the `standardize_features` function that I provided earlier. Here's how you can do it:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def standardize_features(X):\n",
    "    \"\"\"\n",
    "    Standardize the features of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Input data with shape (num_samples, num_features).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Standardized data with shape (num_samples, num_features).\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    standardized_X = (X - mean) / std\n",
    "    return standardized_X\n",
    "\n",
    "def knn_classifier(X_train, y_train, X_test, k=3):\n",
    "    \"\"\"\n",
    "    K-nearest neighbors (KNN) classifier.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (numpy.ndarray): Training data with shape (num_train_samples, num_features).\n",
    "    y_train (numpy.ndarray): Training labels with shape (num_train_samples,).\n",
    "    X_test (numpy.ndarray): Test data with shape (num_test_samples, num_features).\n",
    "    k (int): Number of nearest neighbors to consider.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Predicted labels for the test data.\n",
    "    \"\"\"\n",
    "    standardized_X_train = standardize_features(X_train)\n",
    "    standardized_X_test = standardize_features(X_test)\n",
    "    \n",
    "    num_test_samples = X_test.shape[0]\n",
    "    predicted_labels = np.zeros(num_test_samples)\n",
    "\n",
    "    for i in range(num_test_samples):\n",
    "        distances = np.sqrt(np.sum((standardized_X_train - standardized_X_test[i])**2, axis=1))\n",
    "        nearest_indices = np.argsort(distances)[:k]\n",
    "        nearest_labels = y_train[nearest_indices]\n",
    "        predicted_labels[i] = np.bincount(nearest_labels).argmax()\n",
    "\n",
    "    return predicted_labels\n",
    "```\n",
    "\n",
    "In this implementation, the `standardize_features` function is used to standardize the training and test data before applying the KNN classifier. The `knn_classifier` function takes the training data `X_train`, training labels `y_train`, test data `X_test`, and the number of nearest neighbors `k` as inputs. It then returns the predicted labels for the test data using the KNN algorithm.\n",
    "\n",
    "Remember to call `knn_classifier` with appropriate data and labels to obtain the predicted labels for your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf95673-8a05-4a8b-99cb-c397cc01f1d9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
