{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f255b0-c125-43c3-ba51-a11c55a2165f",
   "metadata": {},
   "source": [
    "# #Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bdf52-4ab9-4692-bbea-8b60c46df6b2",
   "metadata": {},
   "source": [
    "Boosting is a popular ensemble learning technique in machine learning that aims to improve the predictive performance of weak learners (usually referred to as base models) by combining them into a strong learner. The basic idea behind boosting is to iteratively build multiple weak learners and give more weight to the misclassified or poorly predicted examples from the previous models in subsequent iterations.\n",
    "\n",
    "The boosting process can be summarized as follows:\n",
    "\n",
    "Initialization: Each data point in the training set is given an equal weight initially.\n",
    "\n",
    "Iterative Process: The algorithm builds a weak learner (e.g., decision tree with limited depth) on the training data, giving more weight to misclassified or hard-to-predict instances from the previous iteration. In each iteration, the weak learner focuses on the mistakes made by the previous ensemble, attempting to correct them.\n",
    "\n",
    "Weight Update: After each iteration, the weights of correctly classified examples are reduced, while the weights of misclassified examples are increased. This way, the next weak learner will pay more attention to the previously misclassified data points.\n",
    "\n",
    "Final Combination: The weak learners are combined into a strong ensemble model using weighted voting or weighted averaging. The final model, also known as the boosted model, has improved performance compared to the individual weak learners.\n",
    "\n",
    "AdaBoost (Adaptive Boosting) and Gradient Boosting Machines (GBM) are two well-known boosting algorithms. AdaBoost assigns more weight to misclassified examples, whereas GBM builds subsequent models to minimize the residual errors made by the previous models.\n",
    "\n",
    "Boosting is effective in reducing bias and variance, often leading to superior generalization performance compared to individual weak learners. However, it is important to be cautious about overfitting, as boosting can sometimes lead to overfitting on noisy data if not controlled properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab5aa31-4295-46d3-9b43-8022919c9ac4",
   "metadata": {},
   "source": [
    "# #Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e71a1-4d6f-4b47-b0c9-01b0307ac293",
   "metadata": {},
   "source": [
    "Boosting techniques offer several advantages in machine learning, but they also come with certain limitations. Let's explore both aspects:\n",
    "\n",
    "Advantages of Boosting Techniques:\n",
    "\n",
    "Improved Predictive Accuracy: Boosting can significantly enhance the predictive accuracy of models, especially when using weak learners. By combining multiple weak learners into a strong ensemble, boosting can capture complex patterns in the data and reduce both bias and variance.\n",
    "\n",
    "Handles Complex Relationships: Boosting can effectively handle complex relationships and interactions among features in the data, making it suitable for various machine learning tasks, including classification, regression, and ranking problems.\n",
    "\n",
    "Feature Importance: Many boosting algorithms provide information about feature importance, allowing us to identify the most relevant features for making predictions. This can aid in feature selection and interpretability.\n",
    "\n",
    "Robustness to Overfitting: Boosting reduces the risk of overfitting compared to using a single complex model, as it focuses on correcting the mistakes of previous iterations. Regularization techniques are often employed to control overfitting further.\n",
    "\n",
    "Versatility: Boosting can be used with various base learners, such as decision trees, linear models, and neural networks, allowing it to be applied to a wide range of problem domains.\n",
    "\n",
    "Limitations of Boosting Techniques:\n",
    "\n",
    "Computationally Intensive: Boosting algorithms can be computationally expensive, especially when dealing with a large number of weak learners or data with a high dimensionality. This can make training and inference times longer.\n",
    "\n",
    "Sensitivity to Noisy Data: Boosting tends to be sensitive to noisy data, and the misclassification of noisy examples in early iterations can propagate and negatively affect the final model's performance.\n",
    "\n",
    "Potential Overfitting: Although boosting can help reduce overfitting compared to using individual complex models, it is still possible to overfit if the boosting process is not appropriately tuned or if the weak learners are too complex.\n",
    "\n",
    "Parameter Tuning: Boosting algorithms typically have hyperparameters that need to be carefully tuned to achieve optimal performance. Improper parameter settings can lead to suboptimal results.\n",
    "\n",
    "Data Imbalance: Boosting can be affected by class imbalances in the data. If one class is significantly underrepresented, the boosting process might be biased towards the majority class, leading to lower performance for the minority class.\n",
    "\n",
    "Despite these limitations, boosting remains a powerful and widely used technique in machine learning, especially in scenarios where high predictive accuracy and robustness are essential. To address some of the limitations, it is common to use techniques such as cross-validation, early stopping, and regularization during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e50d2-2137-4ed2-a906-f2f7d05c3bac",
   "metadata": {},
   "source": [
    "# #Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2c68e-769c-4651-bd23-b66c760afaa0",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique that combines multiple weak learners (usually referred to as base models or weak classifiers) to create a strong learner. The core idea behind boosting is to iteratively build weak learners, giving more importance to the misclassified instances or the hard-to-predict examples in subsequent iterations. The final model, known as the boosted model or the ensemble, is a weighted combination of the weak learners.\n",
    "\n",
    "Here is a step-by-step explanation of how boosting works:\n",
    "\n",
    "Initialization: Each data point in the training set is assigned an equal weight (initially, the weights are normalized to sum up to 1). The weights represent the importance of each data point in the subsequent iterations.\n",
    "\n",
    "Iterative Process:\n",
    "a. The boosting algorithm builds a weak learner (e.g., a decision tree with limited depth) on the training data. This weak learner is constructed to minimize the error with respect to the weighted samples. In the first iteration, all data points have equal weights, so the weak learner tries to classify the examples with an emphasis on minimizing overall error.\n",
    "\n",
    "b. The weak learner is then evaluated on the training data. Misclassified examples are given higher weights, whereas correctly classified examples have their weights reduced. The goal is to focus the attention of the subsequent weak learners on the misclassified data points from the previous iteration.\n",
    "\n",
    "Weight Update:\n",
    "a. The weights of correctly classified examples are reduced. The intuition is that these examples are relatively easy to classify and do not require much attention in the next iteration.\n",
    "b. The weights of misclassified examples are increased. The weak learner's mistakes are emphasized so that the next learner will focus more on these challenging examples.\n",
    "\n",
    "Combining Weak Learners:\n",
    "a. After each iteration, the weights of the training examples have been updated based on their classification performance. The next iteration then builds another weak learner, with the new weights emphasizing the importance of the previously misclassified examples.\n",
    "b. This process continues for a predetermined number of iterations or until a specific stopping criterion is met.\n",
    "\n",
    "Final Combination:\n",
    "a. Once all the weak learners are constructed, their predictions are combined into a final ensemble model. The combination is often done using weighted voting (for classification) or weighted averaging (for regression), where each weak learner's contribution is determined by its performance and the weights of the examples it focused on during training.\n",
    "\n",
    "Prediction:\n",
    "a. For new, unseen data, the final boosted model makes predictions by aggregating the predictions of all the weak learners, considering their respective weights.\n",
    "\n",
    "The boosting process helps improve the model's performance by iteratively refining the weak learners' focus on the most challenging examples in the training data. This way, the ensemble captures complex patterns and generalizes better than using individual weak learners alone. AdaBoost (Adaptive Boosting) and Gradient Boosting Machines (GBM) are two popular boosting algorithms used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba77f7-4c1a-4e5a-9996-22a7305b5566",
   "metadata": {},
   "source": [
    "# #Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549802d6-fb41-4089-8e15-da2088b5df53",
   "metadata": {},
   "source": [
    "There are several types of boosting algorithms, each with its unique characteristics and variations. Some of the most popular boosting algorithms are:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting)**: AdaBoost is one of the earliest and most well-known boosting algorithms. It works by iteratively building weak learners and giving higher weights to misclassified examples in each iteration. The final model is a weighted combination of these weak learners. AdaBoost is primarily used for binary classification tasks but can be extended to multi-class classification and regression problems.\n",
    "\n",
    "2. **Gradient Boosting Machines (GBM)**: GBM is a more general boosting algorithm that can handle both classification and regression tasks. It builds weak learners sequentially, with each one attempting to minimize the loss function's negative gradient (hence the name \"Gradient Boosting\"). Popular implementations of GBM include XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "3. **Extreme Gradient Boosting (XGBoost)**: XGBoost is an optimized and highly efficient implementation of gradient boosting. It incorporates several techniques to speed up the training process and improve performance, such as regularization, parallel processing, and tree pruning. XGBoost has become popular in various machine learning competitions and real-world applications due to its scalability and accuracy.\n",
    "\n",
    "4. **LightGBM**: LightGBM is another gradient boosting framework designed to be memory-efficient and faster than traditional gradient boosting algorithms. It uses a histogram-based approach to bin continuous features and reduces the memory footprint. LightGBM is particularly useful when dealing with large datasets and high-dimensional feature spaces.\n",
    "\n",
    "5. **CatBoost**: CatBoost is a gradient boosting library that is specifically designed to handle categorical features efficiently. It automatically handles the encoding of categorical variables and provides better handling of out-of-the-box data. CatBoost is robust to overfitting and often requires less hyperparameter tuning.\n",
    "\n",
    "6. **Stochastic Gradient Boosting**: This variant of gradient boosting introduces randomness into the training process by using subsets (random subsets or subsets sampled with replacement) of the training data in each iteration. This randomness can improve generalization and reduce overfitting.\n",
    "\n",
    "7. **Histogram-Based Gradient Boosting**: Some boosting algorithms, like LightGBM, use histogram-based techniques for building trees instead of traditional approaches like exact greedy algorithms or approximate algorithms. Histogram-based methods can significantly speed up the training process and reduce memory usage.\n",
    "\n",
    "8. **LogitBoost**: LogitBoost is a boosting algorithm specifically designed for binary classification problems. It optimizes the log likelihood of a logistic regression model in each iteration, which makes it well-suited for probabilistic classification tasks.\n",
    "\n",
    "Each of these boosting algorithms has its strengths and weaknesses, and the choice of the appropriate algorithm often depends on the specific characteristics of the dataset and the problem at hand. It is essential to experiment with different algorithms and tune their hyperparameters to achieve the best performance for a particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d4c95-22c7-49eb-ad7e-44457166e079",
   "metadata": {},
   "source": [
    "# #Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7f309-b5f6-4203-a845-920b9355bbe7",
   "metadata": {},
   "source": [
    "Boosting algorithms are a family of machine learning techniques that aim to combine weak learners (usually decision trees) to create a strong learner. Common parameters in boosting algorithms may vary depending on the specific algorithm, but some general parameters that are commonly found include:\n",
    "\n",
    "1. **Number of estimators (n_estimators)**: This parameter specifies the number of weak learners (usually decision trees) to be combined in the boosting process. Increasing the number of estimators can lead to better performance but also increases computation time.\n",
    "\n",
    "2. **Learning rate (or shrinkage)**: The learning rate controls the contribution of each weak learner to the final prediction. A lower learning rate requires more estimators to achieve similar performance, but it can improve generalization and robustness.\n",
    "\n",
    "3. **Maximum depth (max_depth)**: In boosting algorithms based on decision trees (e.g., AdaBoost, Gradient Boosting), this parameter determines the maximum depth allowed for each weak learner. It controls the complexity of the individual trees and can help prevent overfitting.\n",
    "\n",
    "4. **Subsample ratio (subsample)**: This parameter specifies the fraction of the training data used to train each weak learner. Setting it to a value less than 1.0 can help reduce overfitting and speed up the training process.\n",
    "\n",
    "5. **Loss function (or objective)**: The loss function defines the measure of error used during the training process. Different boosting algorithms may use different loss functions tailored to specific tasks (e.g., regression, classification).\n",
    "\n",
    "6. **Feature subsampling (colsample_bytree, colsample_bylevel)**: For boosting algorithms that use decision trees, these parameters control the fraction of features (columns) randomly selected for building each tree or each level of a tree. It can help to reduce overfitting and enhance diversity among weak learners.\n",
    "\n",
    "7. **Regularization parameters**: Some boosting algorithms offer regularization parameters (e.g., L1 regularization, L2 regularization) to control model complexity and prevent overfitting.\n",
    "\n",
    "8. **Base estimator**: The type of weak learner used in the boosting algorithm can be specified, such as decision trees, linear models, or neural networks.\n",
    "\n",
    "It's important to note that these parameters may have different names or variations across different implementations of boosting algorithms (e.g., AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM, CatBoost). Always refer to the specific documentation of the library or package you are using for precise parameter names and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61280f2-c787-45b8-b273-a192e3d741e2",
   "metadata": {},
   "source": [
    "# #Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dd64b-9874-4d78-9c94-d26463f6f8f6",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique that combines multiple weak learners to create a strong learner. The main idea behind boosting is to iteratively train weak learners and give more weight to misclassified instances, so subsequent weak learners focus on those examples and improve the overall performance of the ensemble.\n",
    "\n",
    "Here's a general outline of how boosting algorithms work to create a strong learner:\n",
    "\n",
    "1. **Initialize weights:** At the beginning, each instance in the training dataset is assigned equal weight (or normalized weights summing to 1).\n",
    "\n",
    "2. **Train weak learners:** A weak learner is typically a simple model that performs slightly better than random guessing. Examples of weak learners include decision stumps (single-level decision trees), shallow decision trees, or small neural networks with few hidden layers. Each weak learner is trained on the training dataset, where the weight of each instance influences the learning process.\n",
    "\n",
    "3. **Evaluate weak learners:** After training, each weak learner is evaluated on the training data, and its performance is assessed based on some evaluation metric (e.g., accuracy, error rate).\n",
    "\n",
    "4. **Weighted voting:** The weak learners' predictions are combined using weighted voting, where each weak learner's contribution to the final prediction is determined by its performance on the training data. Weak learners with better performance are given higher weights, indicating they have more influence on the final prediction.\n",
    "\n",
    "5. **Update instance weights:** Misclassified instances are given higher weights to make them more important in the next iteration. This means that in subsequent iterations, the weak learners will focus more on these misclassified instances, attempting to correct their mistakes.\n",
    "\n",
    "6. **Iterate:** Steps 2 to 5 are repeated for a predefined number of iterations or until a stopping criterion is met. Each iteration aims to improve the overall performance of the ensemble.\n",
    "\n",
    "7. **Final prediction:** The final prediction is made by aggregating the predictions of all weak learners, typically using weighted voting again.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), and XGBoost. These algorithms differ in the way they assign weights to instances, update weights in each iteration, and how they handle multiclass problems. However, the general concept of iteratively combining weak learners remains consistent across all boosting techniques. Boosting has proven to be a powerful and effective method for creating strong learners that can achieve high accuracy and robustness in various machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfc41b-fb9e-44ff-aeb7-80156e97bbf6",
   "metadata": {},
   "source": [
    "# #Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf14005-2532-4144-9cee-2b7f456988fd",
   "metadata": {},
   "source": [
    "AdaBoost, short for Adaptive Boosting, is a popular ensemble learning algorithm that aims to improve the performance of weak learners (usually decision trees or stumps) by combining them into a strong learner. The core idea behind AdaBoost is to give more weight to misclassified instances in each iteration, making subsequent weak learners focus on the previously misclassified examples and correct their errors. AdaBoost is a sequential learning algorithm, where each weak learner is built based on the mistakes of its predecessors.\n",
    "\n",
    "Here's a step-by-step explanation of how the AdaBoost algorithm works:\n",
    "\n",
    "1. **Initialize instance weights:** At the beginning, each instance in the training dataset is assigned equal weight (or normalized weights summing to 1).\n",
    "\n",
    "2. **Train weak learners:** The first weak learner is trained on the original training data, where each instance's weight influences the learning process. The weak learner aims to minimize the weighted error, where misclassified instances receive higher weights.\n",
    "\n",
    "3. **Evaluate weak learner:** After training, the weak learner's performance is assessed on the training data, and its weighted error (weighted misclassification rate) is calculated.\n",
    "\n",
    "4. **Calculate weak learner weight:** Based on the weighted error, a weight (alpha) is assigned to the weak learner. High accuracy leads to a higher weight, indicating a stronger influence on the final prediction.\n",
    "\n",
    "5. **Update instance weights:** The weights of the misclassified instances are increased to make them more important in the next iteration. This way, the subsequent weak learners focus more on these misclassified instances and try to correct their mistakes.\n",
    "\n",
    "6. **Normalize instance weights:** After updating the instance weights, they are normalized so that they sum up to 1 again.\n",
    "\n",
    "7. **Iterate:** Steps 2 to 6 are repeated for a predefined number of iterations (controlled by the number of weak learners) or until a stopping criterion is met.\n",
    "\n",
    "8. **Final prediction:** The final prediction is made by combining the predictions of all weak learners using weighted voting. The stronger weak learners (with higher alpha weights) have a greater say in the final prediction.\n",
    "\n",
    "The AdaBoost algorithm's final prediction is a weighted sum of the individual weak learners' predictions. The stronger weak learners, which have higher alpha weights, contribute more to the final prediction, while the weaker ones have less influence. By iteratively focusing on the misclassified instances and adjusting the instance weights, AdaBoost can build a strong learner that achieves high accuracy on the training data and generalizes well to unseen data.\n",
    "\n",
    "One of the advantages of AdaBoost is that it is less prone to overfitting compared to training a single strong learner. It is widely used in practice and has been proven to be effective in various machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c654dc-bb8e-4737-885d-64ce0acc5889",
   "metadata": {},
   "source": [
    "# #Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edd8f8-8031-40ea-84c0-c514d585cbb1",
   "metadata": {},
   "source": [
    "In AdaBoost (Adaptive Boosting) algorithm, the loss function used is the exponential loss function (also known as the exponential error). The exponential loss function is designed to assign higher weights to misclassified samples, which makes it particularly well-suited for boosting algorithms.\n",
    "\n",
    "The exponential loss function is defined as follows:\n",
    "\n",
    "L(y, f(x)) = exp(-y * f(x))\n",
    "\n",
    "where:\n",
    "- L(y, f(x)): The exponential loss for a given sample with true label y and predicted value f(x).\n",
    "- y: The true label of the sample (either +1 or -1, typically).\n",
    "- f(x): The weighted sum of weak learners' predictions for the sample x.\n",
    "\n",
    "In AdaBoost, the algorithm aims to iteratively build a strong learner (a combination of weak learners) by giving more weight to misclassified samples in each iteration. The weak learners are trained to minimize the exponential loss function, and the combination of these weak learners results in a strong classifier that performs well even on complex tasks.\n",
    "\n",
    "The update of sample weights in AdaBoost is done based on the exponential loss, with misclassified samples getting higher weights to ensure that subsequent weak learners focus more on these difficult-to-classify samples. The process continues iteratively until the desired number of weak learners is reached or until the model's performance plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4eb2c-fb10-442f-9eb5-7d93641e925d",
   "metadata": {},
   "source": [
    "# #Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501f62c-126a-406e-a54c-18f260cb770a",
   "metadata": {},
   "source": [
    "In AdaBoost (Adaptive Boosting) algorithm, the loss function used is the exponential loss function (also known as the exponential error). The exponential loss function is designed to assign higher weights to misclassified samples, which makes it particularly well-suited for boosting algorithms.\n",
    "\n",
    "The exponential loss function is defined as follows:\n",
    "\n",
    "L(y, f(x)) = exp(-y * f(x))\n",
    "\n",
    "where:\n",
    "- L(y, f(x)): The exponential loss for a given sample with true label y and predicted value f(x).\n",
    "- y: The true label of the sample (either +1 or -1, typically).\n",
    "- f(x): The weighted sum of weak learners' predictions for the sample x.\n",
    "\n",
    "In AdaBoost, the algorithm aims to iteratively build a strong learner (a combination of weak learners) by giving more weight to misclassified samples in each iteration. The weak learners are trained to minimize the exponential loss function, and the combination of these weak learners results in a strong classifier that performs well even on complex tasks.\n",
    "\n",
    "The update of sample weights in AdaBoost is done based on the exponential loss, with misclassified samples getting higher weights to ensure that subsequent weak learners focus more on these difficult-to-classify samples. The process continues iteratively until the desired number of weak learners is reached or until the model's performance plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a36bf-004c-4d04-a67f-079891a86db4",
   "metadata": {},
   "source": [
    "# #Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8237ea-e0fd-4134-a0d6-836ca91c8345",
   "metadata": {},
   "source": [
    "Increasing the number of estimators (or weak learners) in the AdaBoost algorithm can have both positive and negative effects, depending on the specific dataset and the complexity of the problem being solved. Here are the main effects of increasing the number of estimators:\n",
    "\n",
    "1. Improved accuracy: In general, increasing the number of estimators tends to improve the overall accuracy of the AdaBoost model. As more weak learners are added, the model becomes more expressive and can better capture complex patterns in the data, reducing both bias and variance.\n",
    "\n",
    "2. Reduced overfitting: AdaBoost is a boosting algorithm that has the potential to overfit if the number of estimators is too high. However, in practice, it tends to be less prone to overfitting compared to other algorithms like decision trees. By tuning the number of estimators carefully, one can often find a sweet spot that achieves good generalization performance without overfitting.\n",
    "\n",
    "3. Longer training time: As the number of estimators increases, the training time of the AdaBoost algorithm also increases. Each additional estimator requires training on the entire dataset, which can become computationally expensive for large datasets or complex weak learners.\n",
    "\n",
    "4. Increased memory usage: With more estimators, the memory requirements of the model will also increase. The model needs to store information about each estimator and their respective weights, which can become significant when using a large number of weak learners.\n",
    "\n",
    "5. Robustness to noise: AdaBoost can be sensitive to noisy data, and increasing the number of estimators may lead to increased sensitivity. If the dataset contains noisy or outlier data points, the algorithm might assign high importance to them in an attempt to correct misclassifications, which can negatively impact the overall performance.\n",
    "\n",
    "6. Smoother decision boundaries: In the case of classification tasks, as the number of estimators increases, the decision boundary created by AdaBoost tends to become smoother and more refined. This can be advantageous in certain scenarios where complex and irregular decision boundaries are not desired.\n",
    "\n",
    "To find the optimal number of estimators for a specific problem, one can use techniques like cross-validation or learning curves. Cross-validation helps in selecting the number of estimators that yields the best generalization performance, avoiding overfitting. Learning curves can provide insights into the trade-off between model complexity and dataset size, helping to choose an appropriate number of estimators to achieve the desired balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6790a-5cf4-442f-84d2-0f9449566d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
